{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting textblob\n",
      "  Downloading textblob-0.13.0-py2.py3-none-any.whl (631kB)\n",
      "Requirement already satisfied: nltk>=3.1 in c:\\users\\green\\anaconda3\\envs\\py35\\lib\\site-packages (from textblob)\n",
      "Requirement already satisfied: six in c:\\users\\green\\anaconda3\\envs\\py35\\lib\\site-packages (from nltk>=3.1->textblob)\n",
      "Installing collected packages: textblob\n",
      "Successfully installed textblob-0.13.0\n"
     ]
    }
   ],
   "source": [
    "!pip install textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import numpy as np\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils.np_utils import to_categorical\n",
    "from keras.layers import Dense, Input, Flatten, Convolution1D, Dropout\n",
    "from keras.layers import Conv1D, MaxPooling1D, Embedding\n",
    "from keras.models import Model, Sequential, load_model\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import re\n",
    "from sklearn.metrics import f1_score\n",
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv('train.csv')\n",
    "test = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# function to clean data\n",
    "stops = set(stopwords.words(\"english\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cleanData(text):\n",
    "    txt = str(text)\n",
    "    txt = re.sub(r'[^A-Za-z0-9\\s]', r'', txt)\n",
    "    txt = re.sub(r'\\n', r' ', txt)\n",
    "    txt = \" \".join([w for w in txt.split() if w not in stops])\n",
    "    temp = re.sub(r'[^A-Z]', r' ', txt)\n",
    "    txt = \" \".join([w.lower() for w in txt.split() if w not in temp])\n",
    "    txt = temp + \" \" + txt\n",
    "    txt = ' '.join(word for word in txt.split() if len(word)>1)\n",
    "    st = PorterStemmer()\n",
    "    txt = \" \".join([st.stem(w) for w in txt.split()])\n",
    "    return txt\n",
    "\n",
    "def sentenceSentiment(text):\n",
    "    txt=text.split('.')\n",
    "    pol=0\n",
    "    subj=0\n",
    "    for i in range(len(txt)):\n",
    "        temp=TextBlob(txt[i])\n",
    "        pol=pol+temp.sentiment[0]\n",
    "        subj=subj+temp.sentiment[1]\n",
    "    return [pol,subj]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\green\\Anaconda3\\envs\\py35\\lib\\site-packages\\ipykernel\\__main__.py:8: DeprecationWarning: \n",
      ".ix is deprecated. Please use\n",
      ".loc for label based indexing or\n",
      ".iloc for positional indexing\n",
      "\n",
      "See the documentation here:\n",
      "http://pandas.pydata.org/pandas-docs/stable/indexing.html#ix-indexer-is-deprecated\n"
     ]
    }
   ],
   "source": [
    "## join data\n",
    "test['Is_Response'] = np.nan\n",
    "alldata = pd.concat([train, test]).reset_index(drop=True)\n",
    "y_alldata = [1 if x == 'happy' else 0 for x in alldata['Is_Response']]\n",
    "polarity=[]\n",
    "subjectivity=[]\n",
    "for i in range(len(alldata)):\n",
    "    temp= sentenceSentiment(alldata.ix[i,'Description'])\n",
    "    polarity.append(temp[0])\n",
    "    subjectivity.append(temp[1])\n",
    "\n",
    "alldata['Description'] = alldata['Description'].map(lambda x: cleanData(x))\n",
    "alldata['polarity']=polarity\n",
    "alldata['subjectivity']=subjectivity\n",
    "\n",
    "cols = ['Browser_Used','Device_Used']\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "for x in cols:\n",
    "    lbl = LabelEncoder()\n",
    "    alldata[x] = lbl.fit_transform(alldata[x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MAX_SEQUENCE_LENGTH = 1000\n",
    "MAX_NB_WORDS = 20000\n",
    "EMBEDDING_DIM = 100\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Indexing word vectors.')\n",
    "\n",
    "embeddings_index = {}\n",
    "f = open('glove.6B.100d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Found %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "texts = np.array(alldata['Description'])  # list of text samples\n",
    "labels = [0.0,1.0]  # list of label ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Found %s texts.' % len(texts))\n",
    "\n",
    "# vectorizing the text samples into a 2D integer tensor\n",
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "sequences = tokenizer.texts_to_sequences(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens.' % len(word_index))\n",
    "\n",
    "data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df=data\n",
    "df = pd.DataFrame(df)\n",
    "df.columns = ['col'+ str(x) for x in df.columns]\n",
    "df_all=alldata[cols]\n",
    "df_all=pd.get_dummies(df_all, columns=['Device_Used', 'Browser_Used'])\n",
    "df_all.drop(['Browser_Used_9','Browser_Used_10'], axis=1, inplace=True) # less than 1% \n",
    "df_all['polarity']=alldata['polarity']\n",
    "df_all['subjectivity']=alldata['subjectivity']\n",
    "df = pd.concat([df_all*100, df], axis = 1)\n",
    "labels = to_categorical(np.asarray(labels))\n",
    "print('Shape of data tensor:', df.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "X_train = np.array(df[:int(0.95*len(train))])\n",
    "X_dev= np.array(df[int(0.95*len(train)):int(len(train))])\n",
    "X_test= np.array(df[int(len(train)):])\n",
    "target = y_alldata\n",
    "Y_train = target[:int(0.95*len(train))]\n",
    "Y_dev = target[int((0.95*len(train))):int(len(train))]\n",
    "Y_test = target[int(len(train)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# prepare embedding matrix\n",
    "num_words = len(word_index)\n",
    "embedding_matrix = np.zeros((num_words + 1, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if i >= MAX_NB_WORDS:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Preparing Model!\")\n",
    "\n",
    "# best model\n",
    "model = Sequential()\n",
    "model.add(Embedding(MAX_NB_WORDS, EMBEDDING_DIM, input_length=df.shape[1]))\n",
    "model.add(Conv1D(nb_filter=EMBEDDING_DIM, filter_length=5, border_mode='same', activation='relu'))\n",
    "model.add(MaxPooling1D(2))\n",
    "model.add(Flatten())\n",
    "model.add(Dropout(0.9))\n",
    "model.add(Dense(512, activation='relu'))\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['acc'])\n",
    "\n",
    "flag=True\n",
    "BATCH=128\n",
    "while flag:\n",
    "    if BATCH==2048 break\n",
    "    model.fit(X_train, Y_train, validation_data=(X_dev, Y_dev), nb_epoch=1, batch_size=BATCH) # epochs=3, batch=128+512+2048\n",
    "    scores = model.evaluate(X_dev, Y_dev, verbose=0)\n",
    "    print(\"Accuracy: %.5f%%\" % (scores[1]*100))\n",
    "    BATCH=BATCH*4\n",
    "    \n",
    "model.save('model.h5')\n",
    "\n",
    "def to_labels(x):\n",
    "    if x > 0.5:  # cutoff\n",
    "        return \"happy\"\n",
    "    return \"not_happy\"\n",
    "\n",
    "submission = model.predict(X_test)\n",
    "sub=[]\n",
    "for i in range(len(submission)):\n",
    "    sub.append(submission[i][0])\n",
    "\n",
    "submission_data = pd.DataFrame({'User_ID':test.User_ID, 'Is_Response':sub})\n",
    "submission_data['Is_Response'] = submission_data['Is_Response'].map(lambda x: to_labels(x))\n",
    "submission_data = submission_data[['User_ID','Is_Response']]\n",
    "submission_data.to_csv(\"submission.csv\", index=False) # 0.88 scoreâ€‹"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
